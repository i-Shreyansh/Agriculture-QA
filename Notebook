{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9944743,"sourceType":"datasetVersion","datasetId":6114985}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-20T06:24:05.612130Z","iopub.execute_input":"2024-11-20T06:24:05.612389Z","iopub.status.idle":"2024-11-20T06:24:06.682349Z","shell.execute_reply.started":"2024-11-20T06:24:05.612363Z","shell.execute_reply":"2024-11-20T06:24:06.681427Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# System Configs","metadata":{}},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T11:14:49.730200Z","iopub.execute_input":"2024-11-19T11:14:49.730566Z","iopub.status.idle":"2024-11-19T11:14:50.848050Z","shell.execute_reply.started":"2024-11-19T11:14:49.730523Z","shell.execute_reply":"2024-11-19T11:14:50.847209Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Check if GPU is available\n  # Check if TensorFlow detects a GPU\n    print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n    print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))\nelse:\n    print(\"No GPU found.\")\n    gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    print(f\"GPUs available: {[gpu.name for gpu in gpus]}\")\n  \n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T18:15:26.839341Z","iopub.status.idle":"2024-11-19T18:15:26.839773Z","shell.execute_reply.started":"2024-11-19T18:15:26.839536Z","shell.execute_reply":"2024-11-19T18:15:26.839558Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.__version__)  # PyTorch version\nprint(torch.version.cuda)  # CUDA version used by PyTorch\nprint(torch.cuda.is_available())  # Check if GPU is accessible\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T11:22:17.349192Z","iopub.execute_input":"2024-11-19T11:22:17.349900Z","iopub.status.idle":"2024-11-19T11:22:17.355132Z","shell.execute_reply.started":"2024-11-19T11:22:17.349866Z","shell.execute_reply":"2024-11-19T11:22:17.354043Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n# !pip install unsloth\n# Also get the latest nightly Unsloth!\n\n    \n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n!pip install bitsandbytes triton unsloth-zoo xformers ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T11:25:27.437196Z","iopub.execute_input":"2024-11-19T11:25:27.437590Z","iopub.status.idle":"2024-11-19T11:25:49.619271Z","shell.execute_reply.started":"2024-11-19T11:25:27.437553Z","shell.execute_reply":"2024-11-19T11:25:49.618216Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python --version\n!pip show torch | grep Version\n!python -m torch.utils.collect_env\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:47:43.033193Z","iopub.execute_input":"2024-11-20T15:47:43.033597Z","iopub.status.idle":"2024-11-20T15:48:24.534121Z","shell.execute_reply.started":"2024-11-20T15:47:43.033562Z","shell.execute_reply":"2024-11-20T15:48:24.532408Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Python 3.10.14\nVersion: 2.4.0+cpu\n/opt/conda/lib/python3.10/runpy.py:126: RuntimeWarning: 'torch.utils.collect_env' found in sys.modules after import of package 'torch.utils', but prior to execution of 'torch.utils.collect_env'; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\nCollecting environment information...\nPyTorch version: 2.4.0+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.6 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: version 3.16.3\nLibc version: glibc-2.31\n\nPython version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64-bit runtime)\nPython platform: Linux-6.6.56+-x86_64-with-glibc2.31\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nByte Order:                           Little Endian\nAddress sizes:                        46 bits physical, 48 bits virtual\nCPU(s):                               4\nOn-line CPU(s) list:                  0-3\nThread(s) per core:                   2\nCore(s) per socket:                   2\nSocket(s):                            1\nNUMA node(s):                         1\nVendor ID:                            GenuineIntel\nCPU family:                           6\nModel:                                79\nModel name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\nStepping:                             0\nCPU MHz:                              2199.998\nBogoMIPS:                             4399.99\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            64 KiB\nL1i cache:                            64 KiB\nL2 cache:                             512 KiB\nL3 cache:                             55 MiB\nNUMA node0 CPU(s):                    0-3\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Mitigation; PTE Inversion\nVulnerability Mds:                    Mitigation; Clear CPU buffers; SMT Host state unknown\nVulnerability Meltdown:               Mitigation; PTI\nVulnerability Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; IBRS; IBPB conditional; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; Clear CPU buffers; SMT Host state unknown\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n\nVersions of relevant libraries:\n[pip3] flake8==7.1.1\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] onnx==1.17.0\n[pip3] optree==0.11.0\n[pip3] pytorch-ignite==0.5.1\n[pip3] pytorch-lightning==2.4.0\n[pip3] torch==2.4.0+cpu\n[pip3] torchaudio==2.4.0+cpu\n[pip3] torchinfo==1.8.0\n[pip3] torchmetrics==1.4.2\n[pip3] torchvision==0.19.0+cpu\n[conda] mkl                       2024.2.2            ha957f24_15    conda-forge\n[conda] numpy                     1.26.4          py310hb13e2d6_0    conda-forge\n[conda] optree                    0.11.0                   pypi_0    pypi\n[conda] pytorch-ignite            0.5.1                    pypi_0    pypi\n[conda] pytorch-lightning         2.4.0                    pypi_0    pypi\n[conda] torch                     2.4.0+cpu                pypi_0    pypi\n[conda] torchaudio                2.4.0+cpu                pypi_0    pypi\n[conda] torchinfo                 1.8.0                    pypi_0    pypi\n[conda] torchmetrics              1.4.2                    pypi_0    pypi\n[conda] torchvision               0.19.0+cpu               pypi_0    pypi\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"\nimport json\nimport pandas as pd\nimport wandb\nfrom unsloth import FastLanguageModel","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:50:04.679914Z","iopub.execute_input":"2024-11-20T15:50:04.680422Z","iopub.status.idle":"2024-11-20T15:50:06.213343Z","shell.execute_reply.started":"2024-11-20T15:50:04.680371Z","shell.execute_reply":"2024-11-20T15:50:06.211810Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'"],"ename":"ModuleNotFoundError","evalue":"No module named 'unsloth'","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"# Data Imports","metadata":{}},{"cell_type":"code","source":"df1 = pd.read_parquet('/kaggle/input/agriculture-qa/train-00000-of-00001.parquet')\ndf2 = pd.read_json(\"/kaggle/input/agriculture-qa/agricult_data.json\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:12:53.097242Z","iopub.execute_input":"2024-11-20T08:12:53.097991Z","iopub.status.idle":"2024-11-20T08:12:53.379264Z","shell.execute_reply.started":"2024-11-20T08:12:53.097954Z","shell.execute_reply":"2024-11-20T08:12:53.378502Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df2 = df2.drop(columns=['instruction'])\ndf2.columns = df1.columns\ndf = pd.concat([df1, df2], ignore_index=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:12:54.429621Z","iopub.execute_input":"2024-11-20T08:12:54.430018Z","iopub.status.idle":"2024-11-20T08:12:54.442578Z","shell.execute_reply.started":"2024-11-20T08:12:54.429989Z","shell.execute_reply":"2024-11-20T08:12:54.441696Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocess","metadata":{}},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T06:39:45.052204Z","iopub.execute_input":"2024-11-20T06:39:45.053029Z","iopub.status.idle":"2024-11-20T06:39:45.057043Z","shell.execute_reply.started":"2024-11-20T06:39:45.052992Z","shell.execute_reply":"2024-11-20T06:39:45.056121Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model, tokenizer = FastLanguageModel.from_pretrained(\n#     model_name = \"unsloth/Meta-Llama-3.1-8B\",\n#     max_seq_length = max_seq_length,\n#     dtype = dtype,\n#     load_in_4bit = load_in_4bit,\n#     # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n# )\n# model.save_pretrained(\"lora_model\") # Local saving\n# tokenizer.save_pretrained(\"lora_model\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T07:53:03.013576Z","iopub.execute_input":"2024-11-20T07:53:03.014634Z","iopub.status.idle":"2024-11-20T07:53:03.019196Z","shell.execute_reply.started":"2024-11-20T07:53:03.014585Z","shell.execute_reply":"2024-11-20T07:53:03.018212Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T07:53:05.480661Z","iopub.execute_input":"2024-11-20T07:53:05.481033Z","iopub.status.idle":"2024-11-20T07:53:29.369622Z","shell.execute_reply.started":"2024-11-20T07:53:05.481001Z","shell.execute_reply":"2024-11-20T07:53:29.368571Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"base_model\") # Local saving\ntokenizer.save_pretrained(\"base_model\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T06:52:46.620879Z","iopub.execute_input":"2024-11-20T06:52:46.621706Z","iopub.status.idle":"2024-11-20T06:53:04.186162Z","shell.execute_reply.started":"2024-11-20T06:52:46.621671Z","shell.execute_reply":"2024-11-20T06:53:04.185227Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T07:55:50.209235Z","iopub.execute_input":"2024-11-20T07:55:50.209617Z","iopub.status.idle":"2024-11-20T07:55:56.752555Z","shell.execute_reply.started":"2024-11-20T07:55:50.209585Z","shell.execute_reply":"2024-11-20T07:55:56.751835Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    \n    inputs       = examples[\"question\"]\n    outputs      = examples[\"answers\"]\n    texts = []\n    for input, output in zip(inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:21:04.681054Z","iopub.execute_input":"2024-11-20T08:21:04.681463Z","iopub.status.idle":"2024-11-20T08:21:04.687257Z","shell.execute_reply.started":"2024-11-20T08:21:04.681429Z","shell.execute_reply":"2024-11-20T08:21:04.686239Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\n# Prepare conversations for formatting\nconversations = [\n    [{\"role\": \"user\", \"content\": row[\"question\"]}, {\"role\": \"assistant\", \"content\": row[\"answers\"]}]\n    for _, row in df.iterrows()\n]\n\n# Create a Hugging Face Dataset\nhf_dataset = Dataset.from_dict({\"conversations\": conversations})\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:32:58.319235Z","iopub.execute_input":"2024-11-20T08:32:58.320075Z","iopub.status.idle":"2024-11-20T08:32:59.585609Z","shell.execute_reply.started":"2024-11-20T08:32:58.320033Z","shell.execute_reply":"2024-11-20T08:32:59.584691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hf_dataset","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:33:10.584514Z","iopub.execute_input":"2024-11-20T08:33:10.585305Z","iopub.status.idle":"2024-11-20T08:33:10.590610Z","shell.execute_reply.started":"2024-11-20T08:33:10.585270Z","shell.execute_reply":"2024-11-20T08:33:10.589712Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\n\n\n# Apply the formatting function\nformatted_dataset = hf_dataset.map(formatting_prompts_func, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:35:19.585727Z","iopub.execute_input":"2024-11-20T08:35:19.586086Z","iopub.status.idle":"2024-11-20T08:35:21.172072Z","shell.execute_reply.started":"2024-11-20T08:35:19.586056Z","shell.execute_reply":"2024-11-20T08:35:21.171197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hf_dataset","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:36:02.820857Z","iopub.execute_input":"2024-11-20T08:36:02.821743Z","iopub.status.idle":"2024-11-20T08:36:02.827122Z","shell.execute_reply.started":"2024-11-20T08:36:02.821695Z","shell.execute_reply":"2024-11-20T08:36:02.826219Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom datasets import Dataset\n\n# Add a new column with the formatted text\ndf['text'] = df.apply(formatting_prompts_func, axis=1)\n\n\ndataset = Dataset.from_pandas(df)\n# dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:14:23.174974Z","iopub.execute_input":"2024-11-20T08:14:23.175572Z","iopub.status.idle":"2024-11-20T08:14:26.638418Z","shell.execute_reply.started":"2024-11-20T08:14:23.175536Z","shell.execute_reply":"2024-11-20T08:14:26.637705Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wandb login\n# !wandb login --relogin","metadata":{"execution":{"iopub.status.busy":"2024-11-20T07:17:23.087083Z","iopub.execute_input":"2024-11-20T07:17:23.087429Z","iopub.status.idle":"2024-11-20T07:17:26.348196Z","shell.execute_reply.started":"2024-11-20T07:17:23.087391Z","shell.execute_reply":"2024-11-20T07:17:26.347025Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = hf_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    \n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        num_train_epochs = 1, # Set this for 1 full training run.\n#         max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"wandb\", # Use this for WandB etc\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:37:50.705010Z","iopub.execute_input":"2024-11-20T08:37:50.705373Z","iopub.status.idle":"2024-11-20T08:37:57.760381Z","shell.execute_reply.started":"2024-11-20T08:37:50.705340Z","shell.execute_reply":"2024-11-20T08:37:57.759569Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:39:00.424860Z","iopub.execute_input":"2024-11-20T08:39:00.425193Z","iopub.status.idle":"2024-11-20T08:39:02.143445Z","shell.execute_reply.started":"2024-11-20T08:39:00.425165Z","shell.execute_reply":"2024-11-20T08:39:02.142612Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:39:18.247105Z","iopub.execute_input":"2024-11-20T08:39:18.247463Z","iopub.status.idle":"2024-11-20T08:39:18.255787Z","shell.execute_reply.started":"2024-11-20T08:39:18.247430Z","shell.execute_reply":"2024-11-20T08:39:18.254923Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\ntokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:39:48.564378Z","iopub.execute_input":"2024-11-20T08:39:48.564742Z","iopub.status.idle":"2024-11-20T08:39:48.572154Z","shell.execute_reply.started":"2024-11-20T08:39:48.564710Z","shell.execute_reply":"2024-11-20T08:39:48.571438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:40:56.718272Z","iopub.execute_input":"2024-11-20T08:40:56.718967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save/Load","metadata":{}},{"cell_type":"code","source":"# Now if you want to load the LoRA adapters we just saved for inference, set False to True:\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True\n\n\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"/kaggle/working/fine-tuned Llama-3.2-3B-Instruct\", # YOUR MODEL YOU USED FOR TRAINING\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T17:05:03.963579Z","iopub.execute_input":"2024-11-20T17:05:03.964004Z","iopub.status.idle":"2024-11-20T17:05:32.950597Z","shell.execute_reply.started":"2024-11-20T17:05:03.963971Z","shell.execute_reply":"2024-11-20T17:05:32.949876Z"},"trusted":true},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.3.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a24415bea4d4191ac65fc5d3b37aa84"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"os.getenv(\"HUGGINGFACE_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T17:28:19.834452Z","iopub.execute_input":"2024-11-20T17:28:19.835212Z","iopub.status.idle":"2024-11-20T17:28:19.841331Z","shell.execute_reply.started":"2024-11-20T17:28:19.835177Z","shell.execute_reply":"2024-11-20T17:28:19.840498Z"},"trusted":true},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'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'"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import huggingface_hub as hf","metadata":{"execution":{"iopub.status.busy":"2024-11-20T17:35:57.027293Z","iopub.execute_input":"2024-11-20T17:35:57.027935Z","iopub.status.idle":"2024-11-20T17:35:57.031657Z","shell.execute_reply.started":"2024-11-20T17:35:57.027900Z","shell.execute_reply":"2024-11-20T17:35:57.030775Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Save to HuggingFace_hub","metadata":{}},{"cell_type":"code","source":"# Push model to the hub\ntoken = os.getenv(\"HUGGINGFACE_TOKEN\")\nmodel.push_to_hub(\"ShuklaShreyansh/Agro-QA\", token=token)\nmodel.push_to_hub_merged(\"ShuklaShreyansh/Agro-QA\", token=token)\ntokenizer.push_to_hub(\"ShuklaShreyansh/Agro-QA\", token=token)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T17:45:44.308541Z","iopub.execute_input":"2024-11-20T17:45:44.308885Z","iopub.status.idle":"2024-11-20T17:47:20.238075Z","shell.execute_reply.started":"2024-11-20T17:45:44.308852Z","shell.execute_reply":"2024-11-20T17:47:20.237378Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fae712c7e4f4219ace2f73dea1e14e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f404c7270cb4338ab4e026b7e52e978"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/ShuklaShreyansh/Agro-QA\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 19.68 out of 31.35 RAM for saving.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28/28 [00:04<00:00,  6.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nUnsloth: Saving /tmp/Agro-QA/pytorch_model-00001-of-00002.bin...\nUnsloth: Saving /tmp/Agro-QA/pytorch_model-00002-of-00002.bin...\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"Done.\nSaved merged model to https://huggingface.co/ShuklaShreyansh/Agro-QA\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# from unsloth.chat_templates import get_chat_template\n\n# tokenizer = get_chat_template(\n#     tokenizer,\n#     chat_template = \"llama-3.1\",\n# )\n# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# messages = [\n#     {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n# ]\n# inputs = tokenizer.apply_chat_template(\n#     messages,\n#     tokenize = True,\n#     add_generation_prompt = True, # Must add for generation\n#     return_tensors = \"pt\",\n# ).to(\"cuda\")\n\n# outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n#                          temperature = 1.5, min_p = 0.1)\n# tokenizer.batch_decode(outputs)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T17:15:59.277789Z","iopub.execute_input":"2024-11-20T17:15:59.278551Z","iopub.status.idle":"2024-11-20T17:15:59.282656Z","shell.execute_reply.started":"2024-11-20T17:15:59.278514Z","shell.execute_reply":"2024-11-20T17:15:59.281686Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is Agriculture?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n                   use_cache = True, temperature = 1.5, min_p = 0.1)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T17:16:39.635699Z","iopub.execute_input":"2024-11-20T17:16:39.636431Z","iopub.status.idle":"2024-11-20T17:16:45.284942Z","shell.execute_reply.started":"2024-11-20T17:16:39.636394Z","shell.execute_reply":"2024-11-20T17:16:45.284237Z"},"trusted":true},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Agriculture is the cultivation of crops and rearing of animals, including their breeding, feeding, and care, for food and other products.<|eot_id|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\n# Load model and tokenizer from Hugging Face Hub\nmodel_name = \"ShuklaShreyansh/Agro-QA\"\n\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"/kaggle/working/fine-tuned Llama-3.2-3B-Instruct\", # YOUR MODEL YOU USED FOR TRAINING\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name,token = token)\nmodel = AutoModelForCausalLM.from_pretrained(model_name ,token=token).to(\"cuda\")\n\n# Inference function\ndef generate_response(input_text):\n    messages = [{\"role\": \"user\", \"content\": input_text}]\n    inputs = tokenizer.apply_chat_template(\n        messages, \n        tokenize=True, \n        add_generation_prompt=True, \n        return_tensors=\"pt\"\n    ).to(\"cuda\")\n\n    # Generate a response\n    output_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        max_new_tokens=128,\n        temperature=1.0,\n        use_cache=True\n    )\n    # Decode the output tokens\n    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return response\n\n# Example usage\nif __name__ == \"__main__\":\n    print(\"Agro-QA Model: Ask your agricultural questions!\")\n    user_input = input(\"Question: \")\n    answer = generate_response(user_input)\n    print(\"\\nAnswer:\", answer)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-21T09:00:34.833913Z","iopub.execute_input":"2024-11-21T09:00:34.834248Z","iopub.status.idle":"2024-11-21T09:01:49.059212Z","shell.execute_reply.started":"2024-11-21T09:00:34.834218Z","shell.execute_reply":"2024-11-21T09:01:48.886372Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec5d58a2719a4cc49993dbaa81da6f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c281bfaba0b40fdbd9d3a90d7f47e4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65bd787808c84f968a05d6a0a75c309c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9356c57f6dd4583a0fdf20cbf11dd5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b93c5c19c26e4bb4b6c15a4867b46a48"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShuklaShreyansh/Agro-QA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name,token \u001b[38;5;241m=\u001b[39m token)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Inference function\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(input_text):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4014\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4005\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4007\u001b[0m     (\n\u001b[1;32m   4008\u001b[0m         model,\n\u001b[1;32m   4009\u001b[0m         missing_keys,\n\u001b[1;32m   4010\u001b[0m         unexpected_keys,\n\u001b[1;32m   4011\u001b[0m         mismatched_keys,\n\u001b[1;32m   4012\u001b[0m         offload_index,\n\u001b[1;32m   4013\u001b[0m         error_msgs,\n\u001b[0;32m-> 4014\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4021\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4033\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4034\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4559\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_msg:\n\u001b[1;32m   4556\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4558\u001b[0m         )\n\u001b[0;32m-> 4559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4562\u001b[0m     archs \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.0.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.0.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.0.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.1.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.1.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.1.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.2.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.2.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.2.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.2.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.3.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.3.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.3.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.3.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.4.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.4.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.4.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.4.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.5.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.5.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.5.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.5.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.6.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.6.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.6.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.6.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.7.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.7.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.7.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.7.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.8.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.8.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.8.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.8.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.9.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.9.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.9.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.9.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.10.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.10.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.10.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.10.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.11.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.11.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.11.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.11.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.12.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.12.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.12.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.12.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.13.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.13.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.13.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.13.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.14.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.14.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.14.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.14.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.15.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.15.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.15.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.15.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.16.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.16.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.16.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.16.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.17.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.17.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.17.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.17.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.18.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.18.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.18.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.18.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.19.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.19.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.19.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.19.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.20.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.20.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.20.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.20.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.21.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.21.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.21.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.21.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.22.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.22.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.22.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.22.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.23.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.23.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.23.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.23.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.24.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.24.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.24.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.24.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.25.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.25.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.25.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.25.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.26.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.26.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.26.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.26.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.27.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.27.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.27.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.27.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.0.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.0.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.0.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.1.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.1.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.1.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.2.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.2.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.2.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.2.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.3.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.3.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.3.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.3.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.4.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.4.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.4.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.4.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.5.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.5.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.5.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.5.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.6.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.6.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.6.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.6.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.7.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.7.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.7.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.7.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.8.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.8.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.8.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.8.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.9.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.9.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.9.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.9.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.10.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.10.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.10.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.10.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.11.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.11.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.11.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.11.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.12.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.12.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.12.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.12.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.13.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.13.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.13.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.13.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.14.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.14.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.14.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.14.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.15.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.15.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.15.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.15.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.16.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.16.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.16.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.16.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.17.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.17.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.17.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.17.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.18.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.18.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.18.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.18.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.19.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.19.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.19.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.19.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.20.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.20.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.20.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.20.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.21.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.21.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.21.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.21.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.22.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.22.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.22.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.22.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.23.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.23.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.23.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.23.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.24.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.24.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.24.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.24.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.25.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.25.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.25.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.25.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.26.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.26.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.26.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.26.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.weight: copying a param with shape torch.Size([1572864, 1]) from checkpoint, the shape in current model is torch.Size([1024, 3072]).\n\tsize mismatch for model.layers.27.self_attn.o_proj.weight: copying a param with shape torch.Size([4718592, 1]) from checkpoint, the shape in current model is torch.Size([3072, 3072]).\n\tsize mismatch for model.layers.27.mlp.gate_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.27.mlp.up_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([8192, 3072]).\n\tsize mismatch for model.layers.27.mlp.down_proj.weight: copying a param with shape torch.Size([12582912, 1]) from checkpoint, the shape in current model is torch.Size([3072, 8192]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"!huggingface-cli login","metadata":{"execution":{"iopub.status.busy":"2024-11-20T18:33:16.113723Z","iopub.execute_input":"2024-11-20T18:33:16.114458Z","iopub.status.idle":"2024-11-20T18:33:30.893702Z","shell.execute_reply.started":"2024-11-20T18:33:16.114418Z","shell.execute_reply":"2024-11-20T18:33:30.892813Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\nEnter your token (input will not be visible): Traceback (most recent call last):\n  File \"/opt/conda/bin/huggingface-cli\", line 8, in <module>\n    sys.exit(main())\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n    service.run()\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 115, in login\n    interpreter_login(new_session=new_session, write_permission=write_permission)\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 191, in interpreter_login\n    token = getpass(\"Enter your token (input will not be visible): \")\n  File \"/opt/conda/lib/python3.10/getpass.py\", line 77, in unix_getpass\n    passwd = _raw_input(prompt, stream, input=input)\n  File \"/opt/conda/lib/python3.10/getpass.py\", line 146, in _raw_input\n    line = input.readline()\n  File \"/opt/conda/lib/python3.10/codecs.py\", line 319, in decode\n    def decode(self, input, final=False):\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\n# Define parameters\nmodel_name = \"HuggingFace/model\"  # Replace with your Hugging Face model path, e.g., \"HuggingFace/llama-3b-instruct\"\nmax_seq_length = 512  # Adjust based on your model configuration\ndtype = \"float16\"  # Adjust based on the precision you want (float16, float32)\nload_in_4bit = False  # Set to True if using 4-bit quantization\n\n# Load model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T09:10:11.350968Z","iopub.execute_input":"2024-11-21T09:10:11.351664Z","iopub.status.idle":"2024-11-21T09:10:11.375925Z","shell.execute_reply.started":"2024-11-21T09:10:11.351632Z","shell.execute_reply":"2024-11-21T09:10:11.374883Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'"],"ename":"ModuleNotFoundError","evalue":"No module named 'unsloth'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"!pip freeze > requirements.txt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}